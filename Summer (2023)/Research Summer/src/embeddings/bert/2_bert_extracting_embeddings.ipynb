{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "           output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocesses text input in a way that BERT can interpret.\n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    \n",
    "    display(tokenized_text)\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    display(indexed_tokens)\n",
    "    \n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    \n",
    "    display(segments_ids)\n",
    "    \n",
    "    # convert inputs to tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    display(tokens_tensor)\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    display(segments_tensor)\n",
    "    return tokenized_text, tokens_tensor, segments_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "      outputs = model(tokens_tensor, segments_tensor)\n",
    "      hidden_states = outputs.hidden_states\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "    # \"token\" is a [12 x 768] tensor\n",
    "    # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"bank\",\n",
    "         \"he eventually sold the shares back to the bank at a premium.\",\n",
    "         \"the bank strongly resisted cutting interest rates.\",\n",
    "         \"the bank will supply and buy back foreign currency.\",\n",
    "         \"the bank is pressing us for repayment of the loan.\",\n",
    "         \"the bank left its lending rates unchanged.\",\n",
    "         \"the river flowed over the bank.\",\n",
    "         \"tall, luxuriant plants grew along the river bank.\",\n",
    "         \"his soldiers were arrayed along the river bank.\",\n",
    "         \"wild flowers adorned the river bank.\",\n",
    "         \"two fox cubs romped playfully on the river bank.\",\n",
    "         \"the jewels were kept in a bank vault.\",\n",
    "         \"you can stow your jewellery away in the bank.\",\n",
    "         \"most of the money was in storage in bank vaults.\",\n",
    "         \"the diamonds are shut away in a bank vault somewhere.\",\n",
    "         \"thieves broke into the bank vault.\",\n",
    "         \"can I bank on your support?\",\n",
    "         \"you can bank on him to hand you a reasonable bill for your services.\",\n",
    "         \"don't bank on your friends to help you out of trouble.\",\n",
    "         \"you can bank on me when you need money.\",\n",
    "         \"i bank on your help.\"\n",
    "         ]\n",
    "from collections import OrderedDict\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "for sentence in sentences:\n",
    "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "  # make ordered dictionary to keep track of the position of each   word\n",
    "  tokens = OrderedDict()\n",
    "  # loop over tokens in sensitive sentence\n",
    "  for token in tokenized_text[1:-1]:\n",
    "    # keep track of position of word and whether it occurs multiple times\n",
    "    if token in tokens:\n",
    "      tokens[token] += 1\n",
    "    else:\n",
    "      tokens[token] = 1\n",
    "  # compute the position of the current token\n",
    "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "    current_index = token_indices[tokens[token]-1]\n",
    "  # get the corresponding embedding\n",
    "    token_vec = list_token_embeddings[current_index]\n",
    "    \n",
    "    # save values\n",
    "    context_tokens.append(token)\n",
    "    context_embeddings.append(token_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Here is my text, some gibberish: sdsdasdaf, and animals, hippos etc. [1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[  101,  2182,  2003,  2026,  3793,  1010,  2070, 21025, 29325,  4509,\n",
      "          1024, 17371, 16150,  3022,  2850,  2546,  1010,  1998,  4176,  1010,\n",
      "          5099,  6873,  2015,  4385,  1012,  1031,  1015,  1033,   102]])\n",
      "torch.Size([1, 29])\n",
      "tensor([[[-0.0725,  0.2891, -0.2205,  ..., -0.3550,  0.5276,  0.5011],\n",
      "         [-1.0338, -0.0576,  0.0736,  ..., -0.3047,  0.4641, -0.0979],\n",
      "         [-0.6666, -0.1912,  0.5514,  ...,  0.2056, -0.1143,  0.3831],\n",
      "         ...,\n",
      "         [-0.1139,  0.2144,  0.5290,  ..., -0.1144,  0.7179,  1.0353],\n",
      "         [ 0.0453,  0.3491,  0.3005,  ..., -0.6518, -0.2924,  0.1984],\n",
      "         [ 0.5270,  0.6090, -0.2633,  ...,  0.1840, -0.5477, -0.1830]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 29, 768])\n",
      "torch.Size([1, 29, 768])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.encode(\"Here is my text, some gibberish: sdsdasdaf, and animals, hippos etc. [1]\", return_tensors=\"pt\")\n",
    "print(type(tokenized))\n",
    "print(tokenized)\n",
    "print(tokenized.shape)\n",
    "embedded = model(input_ids=tokenized)\n",
    "last = embedded.last_hidden_state\n",
    "print(last)\n",
    "print(last.shape)\n",
    "last.squeeze()\n",
    "print(last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[  101,  2182,  2003,  2026,  3793,  1010,  2070, 21025, 29325,  4509,\n",
      "          1024, 17371, 16150,  3022,  2850,  2546,  1010,  1998,  4176,  1010,\n",
      "          5099,  6873,  2015,  4385,  1012,  1031,  1015,  1033,   102]])\n",
      "torch.Size([1, 29])\n",
      "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [-0.3557,  0.9575, -1.3448,  ...,  0.5972,  0.7392,  0.2581],\n",
      "         [-0.6270, -0.0633, -0.3143,  ...,  0.3427,  0.4636,  0.4594],\n",
      "         ...,\n",
      "         [ 0.0116,  0.4963,  0.2872,  ..., -0.4949,  0.7428,  0.3255],\n",
      "         [ 0.7604,  0.6366,  0.2540,  ...,  0.1930,  0.8327, -0.8258],\n",
      "         [-0.4342,  0.1415,  0.2393,  ..., -0.4481, -0.0569, -0.2665]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0399, -0.0126, -0.1802,  ...,  0.2281, -0.1365, -0.0331],\n",
      "         [-0.9734,  0.9356, -1.4833,  ...,  0.1141,  0.7378,  0.3468],\n",
      "         [-1.1578, -0.3954, -0.7789,  ...,  0.1855,  0.3490,  0.2681],\n",
      "         ...,\n",
      "         [-0.1729,  0.6751,  0.0327,  ..., -1.3038,  0.6987,  0.0942],\n",
      "         [ 1.3192,  0.2197,  0.3246,  ...,  0.1738,  0.8448, -1.0358],\n",
      "         [-0.3367,  0.0345, -0.0606,  ..., -0.2758,  0.0766, -0.4245]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.5508e-02, -3.5402e-01, -3.8775e-01,  ...,  4.6752e-01,\n",
      "           7.5740e-02,  6.5760e-04],\n",
      "         [-1.3040e+00,  1.1557e+00, -7.2241e-01,  ..., -2.2510e-01,\n",
      "           1.2874e+00,  1.2210e-02],\n",
      "         [-1.2677e+00, -2.4922e-01, -2.2636e-01,  ...,  2.6193e-01,\n",
      "           2.6394e-01,  1.3383e-01],\n",
      "         ...,\n",
      "         [-8.6035e-01,  7.0894e-01,  3.8020e-01,  ..., -1.1926e+00,\n",
      "           8.6503e-01,  6.7493e-01],\n",
      "         [ 8.7403e-01,  6.7528e-01,  9.8344e-01,  ...,  2.5445e-01,\n",
      "           4.5113e-01, -1.7318e+00],\n",
      "         [-2.8543e-01, -2.2812e-01, -1.2318e-02,  ..., -1.1717e-01,\n",
      "           8.5224e-02, -5.0606e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0307, -0.4184, -0.1715,  ...,  0.4621,  0.1983,  0.1765],\n",
      "         [-1.5824,  0.8913, -0.4903,  ..., -0.2579,  1.1284,  0.2621],\n",
      "         [-0.9959, -0.2516,  0.1648,  ..., -0.0700,  0.4119,  0.3125],\n",
      "         ...,\n",
      "         [-0.8897,  0.8357,  0.6230,  ..., -0.9879,  0.5739,  0.6664],\n",
      "         [ 0.5791,  0.5477,  1.2080,  ...,  0.1642,  0.2361, -1.8402],\n",
      "         [-0.0663, -0.1356,  0.1024,  ...,  0.0565,  0.0244, -0.0677]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2846, -0.7186, -0.4576,  ...,  0.5905,  0.3675,  0.5221],\n",
      "         [-1.5403,  0.6490,  0.3842,  ..., -0.0057,  1.0457, -0.0692],\n",
      "         [-0.7605, -0.1659,  0.2386,  ..., -0.5689,  0.3870,  0.8333],\n",
      "         ...,\n",
      "         [-0.7132,  0.2932,  0.6541,  ..., -1.1875,  0.5762,  1.1123],\n",
      "         [ 0.5685,  0.8548,  0.7491,  ..., -0.2944,  0.0576, -1.7447],\n",
      "         [-0.0148, -0.0600,  0.0149,  ...,  0.0240,  0.0525, -0.0353]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1171, -0.7196, -0.2590,  ...,  0.0164,  0.3137,  0.5532],\n",
      "         [-1.3431,  0.7780,  0.5314,  ..., -0.0075,  0.9622, -0.2355],\n",
      "         [-0.7382, -0.3911,  0.0704,  ..., -0.5522,  0.7142,  1.1660],\n",
      "         ...,\n",
      "         [-0.7237,  0.2257,  0.6153,  ..., -0.6242,  0.2135,  0.9625],\n",
      "         [ 0.1762,  0.4565,  1.5892,  ..., -0.5841,  0.2599, -1.9709],\n",
      "         [-0.0227, -0.0379,  0.0360,  ...,  0.0184,  0.0068, -0.0402]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0897, -1.1075, -0.2328,  ...,  0.0693,  0.5336,  0.4764],\n",
      "         [-1.1713,  0.5526, -0.0378,  ...,  0.1605,  0.8468,  0.1411],\n",
      "         [-1.1466, -0.9828,  0.2055,  ..., -0.4851,  0.6336,  1.1637],\n",
      "         ...,\n",
      "         [-1.2477, -0.1433,  0.7816,  ..., -0.2434,  0.4398,  0.3943],\n",
      "         [ 0.1234,  0.6319,  1.2700,  ..., -0.3341, -0.3376, -1.2437],\n",
      "         [ 0.0183, -0.0463, -0.0025,  ..., -0.0024, -0.0101, -0.0321]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2353, -1.1719, -0.3116,  ..., -0.2151,  0.3802,  0.5432],\n",
      "         [-0.9039,  0.9727, -0.4387,  ...,  0.3021,  0.9193, -0.0339],\n",
      "         [-1.2058, -0.6639,  0.1075,  ..., -0.1303,  0.5974,  1.1396],\n",
      "         ...,\n",
      "         [-1.3462,  0.1116,  0.3129,  ..., -0.4286,  0.5918,  0.4569],\n",
      "         [-0.1428,  0.1531, -0.3560,  ..., -0.3080, -0.4239,  0.0182],\n",
      "         [ 0.0218, -0.0073, -0.0193,  ..., -0.0281,  0.0108, -0.0427]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4819, -0.4166, -0.5972,  ..., -0.2471,  0.0656,  0.2539],\n",
      "         [-0.9100,  1.0225, -0.7030,  ..., -0.0890,  0.3947,  0.0882],\n",
      "         [-0.8906, -0.1712, -0.2181,  ...,  0.0521,  0.5320,  1.1701],\n",
      "         ...,\n",
      "         [-0.9832,  0.0844,  0.1095,  ..., -0.1036,  0.4251,  0.0568],\n",
      "         [-0.4155,  0.5327, -0.6365,  ..., -0.5491, -0.8150, -0.4458],\n",
      "         [ 0.0268,  0.0100,  0.0146,  ..., -0.0248, -0.0419, -0.0683]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5282, -0.2739, -0.1988,  ..., -0.0386,  0.2249,  0.2729],\n",
      "         [-1.2943,  0.6637,  0.0184,  ...,  0.4478,  0.5748, -0.1668],\n",
      "         [-0.7364, -0.0855,  0.1593,  ...,  0.3331,  0.8238,  0.6063],\n",
      "         ...,\n",
      "         [-0.7989,  0.3114,  0.2514,  ..., -0.2868,  0.9175,  0.0911],\n",
      "         [-0.4746,  0.5260,  0.0610,  ..., -0.1985, -0.7068, -0.5364],\n",
      "         [-0.0037, -0.0392,  0.0339,  ..., -0.0369, -0.0320, -0.0530]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 2.7113e-01, -4.4476e-01, -1.2701e-01,  ..., -1.9361e-01,\n",
      "           7.3809e-02,  4.4942e-01],\n",
      "         [-1.1372e+00,  3.6119e-01,  3.1553e-01,  ...,  7.1750e-02,\n",
      "           4.4833e-01,  8.8378e-04],\n",
      "         [-6.8136e-01,  1.0931e-01,  8.5238e-01,  ..., -1.3484e-01,\n",
      "           2.7231e-01,  6.9002e-01],\n",
      "         ...,\n",
      "         [-5.8655e-01, -3.8321e-02,  3.1428e-01,  ..., -1.2425e-01,\n",
      "           9.9008e-01,  6.6271e-01],\n",
      "         [-8.2100e-02, -4.4347e-02,  3.5109e-01,  ..., -2.5357e-01,\n",
      "          -6.2404e-01, -4.1493e-01],\n",
      "         [ 6.7865e-03, -3.4195e-02,  1.6905e-01,  ...,  8.0652e-02,\n",
      "          -4.2763e-02,  8.9724e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2684, -0.0333, -0.0149,  ..., -0.1437,  0.2634,  0.4766],\n",
      "         [-1.0747,  0.1052, -0.1669,  ..., -0.3573,  0.3286,  0.1844],\n",
      "         [-0.7387, -0.1070,  0.8522,  ..., -0.2037,  0.2798,  0.8110],\n",
      "         ...,\n",
      "         [-0.4837,  0.0839,  0.2376,  ..., -0.4457,  0.9157,  0.9990],\n",
      "         [ 0.2624,  0.1707,  0.6638,  ..., -0.3752, -0.5839, -0.0608],\n",
      "         [ 0.0410,  0.0215, -0.0013,  ...,  0.0419, -0.0520,  0.0120]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0725,  0.2891, -0.2205,  ..., -0.3550,  0.5276,  0.5011],\n",
      "         [-1.0338, -0.0576,  0.0736,  ..., -0.3047,  0.4641, -0.0979],\n",
      "         [-0.6666, -0.1912,  0.5514,  ...,  0.2056, -0.1143,  0.3831],\n",
      "         ...,\n",
      "         [-0.1139,  0.2144,  0.5290,  ..., -0.1144,  0.7179,  1.0353],\n",
      "         [ 0.0453,  0.3491,  0.3005,  ..., -0.6518, -0.2924,  0.1984],\n",
      "         [ 0.5270,  0.6090, -0.2633,  ...,  0.1840, -0.5477, -0.1830]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>))\n",
      "13\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 29, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
       "            3.8253e-02,  1.6400e-01],\n",
       "          [-3.5574e-01,  9.5751e-01, -1.3448e+00,  ...,  5.9716e-01,\n",
       "            7.3916e-01,  2.5809e-01],\n",
       "          [-6.2703e-01, -6.3313e-02, -3.1428e-01,  ...,  3.4265e-01,\n",
       "            4.6361e-01,  4.5937e-01],\n",
       "          ...,\n",
       "          [ 1.1559e-02,  4.9627e-01,  2.8718e-01,  ..., -4.9486e-01,\n",
       "            7.4276e-01,  3.2547e-01],\n",
       "          [ 7.6038e-01,  6.3660e-01,  2.5401e-01,  ...,  1.9296e-01,\n",
       "            8.3270e-01, -8.2576e-01],\n",
       "          [-4.3419e-01,  1.4149e-01,  2.3928e-01,  ..., -4.4813e-01,\n",
       "           -5.6863e-02, -2.6652e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.9913e-02, -1.2561e-02, -1.8017e-01,  ...,  2.2813e-01,\n",
       "           -1.3654e-01, -3.3117e-02],\n",
       "          [-9.7344e-01,  9.3563e-01, -1.4833e+00,  ...,  1.1412e-01,\n",
       "            7.3782e-01,  3.4675e-01],\n",
       "          [-1.1578e+00, -3.9543e-01, -7.7889e-01,  ...,  1.8548e-01,\n",
       "            3.4905e-01,  2.6808e-01],\n",
       "          ...,\n",
       "          [-1.7290e-01,  6.7510e-01,  3.2739e-02,  ..., -1.3038e+00,\n",
       "            6.9872e-01,  9.4205e-02],\n",
       "          [ 1.3192e+00,  2.1974e-01,  3.2462e-01,  ...,  1.7378e-01,\n",
       "            8.4481e-01, -1.0358e+00],\n",
       "          [-3.3669e-01,  3.4543e-02, -6.0624e-02,  ..., -2.7584e-01,\n",
       "            7.6568e-02, -4.2447e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.5508e-02, -3.5402e-01, -3.8775e-01,  ...,  4.6752e-01,\n",
       "            7.5740e-02,  6.5760e-04],\n",
       "          [-1.3040e+00,  1.1557e+00, -7.2241e-01,  ..., -2.2510e-01,\n",
       "            1.2874e+00,  1.2210e-02],\n",
       "          [-1.2677e+00, -2.4922e-01, -2.2636e-01,  ...,  2.6193e-01,\n",
       "            2.6394e-01,  1.3383e-01],\n",
       "          ...,\n",
       "          [-8.6035e-01,  7.0894e-01,  3.8020e-01,  ..., -1.1926e+00,\n",
       "            8.6503e-01,  6.7493e-01],\n",
       "          [ 8.7403e-01,  6.7528e-01,  9.8344e-01,  ...,  2.5445e-01,\n",
       "            4.5113e-01, -1.7318e+00],\n",
       "          [-2.8543e-01, -2.2812e-01, -1.2318e-02,  ..., -1.1717e-01,\n",
       "            8.5224e-02, -5.0606e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 2.7113e-01, -4.4476e-01, -1.2701e-01,  ..., -1.9361e-01,\n",
       "            7.3809e-02,  4.4942e-01],\n",
       "          [-1.1372e+00,  3.6119e-01,  3.1553e-01,  ...,  7.1750e-02,\n",
       "            4.4833e-01,  8.8378e-04],\n",
       "          [-6.8136e-01,  1.0931e-01,  8.5238e-01,  ..., -1.3484e-01,\n",
       "            2.7231e-01,  6.9002e-01],\n",
       "          ...,\n",
       "          [-5.8655e-01, -3.8321e-02,  3.1428e-01,  ..., -1.2425e-01,\n",
       "            9.9008e-01,  6.6271e-01],\n",
       "          [-8.2100e-02, -4.4347e-02,  3.5109e-01,  ..., -2.5357e-01,\n",
       "           -6.2404e-01, -4.1493e-01],\n",
       "          [ 6.7865e-03, -3.4195e-02,  1.6905e-01,  ...,  8.0652e-02,\n",
       "           -4.2763e-02,  8.9724e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.6841e-01, -3.3346e-02, -1.4946e-02,  ..., -1.4366e-01,\n",
       "            2.6340e-01,  4.7662e-01],\n",
       "          [-1.0747e+00,  1.0520e-01, -1.6687e-01,  ..., -3.5734e-01,\n",
       "            3.2856e-01,  1.8436e-01],\n",
       "          [-7.3866e-01, -1.0704e-01,  8.5215e-01,  ..., -2.0372e-01,\n",
       "            2.7978e-01,  8.1103e-01],\n",
       "          ...,\n",
       "          [-4.8373e-01,  8.3927e-02,  2.3757e-01,  ..., -4.4567e-01,\n",
       "            9.1574e-01,  9.9905e-01],\n",
       "          [ 2.6237e-01,  1.7065e-01,  6.6377e-01,  ..., -3.7517e-01,\n",
       "           -5.8386e-01, -6.0768e-02],\n",
       "          [ 4.0950e-02,  2.1481e-02, -1.2679e-03,  ...,  4.1916e-02,\n",
       "           -5.2026e-02,  1.1988e-02]]],\n",
       "\n",
       "\n",
       "        [[[-7.2516e-02,  2.8913e-01, -2.2054e-01,  ..., -3.5502e-01,\n",
       "            5.2764e-01,  5.0110e-01],\n",
       "          [-1.0338e+00, -5.7566e-02,  7.3614e-02,  ..., -3.0467e-01,\n",
       "            4.6406e-01, -9.7906e-02],\n",
       "          [-6.6662e-01, -1.9117e-01,  5.5137e-01,  ...,  2.0558e-01,\n",
       "           -1.1432e-01,  3.8309e-01],\n",
       "          ...,\n",
       "          [-1.1388e-01,  2.1439e-01,  5.2896e-01,  ..., -1.1444e-01,\n",
       "            7.1795e-01,  1.0353e+00],\n",
       "          [ 4.5329e-02,  3.4912e-01,  3.0053e-01,  ..., -6.5177e-01,\n",
       "           -2.9242e-01,  1.9844e-01],\n",
       "          [ 5.2699e-01,  6.0896e-01, -2.6330e-01,  ...,  1.8403e-01,\n",
       "           -5.4772e-01, -1.8302e-01]]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer.encode(\"Here is my text, some gibberish: sdsdasdaf, and animals, hippos etc. [1]\", return_tensors=\"pt\")\n",
    "print(type(tokenized))\n",
    "print(tokenized)\n",
    "print(tokenized.shape)\n",
    "embedded = model(input_ids=tokenized)\n",
    "last = embedded.hidden_states\n",
    "print(last)\n",
    "print(len(last))\n",
    "for layer in last:\n",
    "    print(type(layer))\n",
    "    print(layer.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 29, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.stack(last, dim=0)\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 29, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[ 101, 2182,  102]])\n",
      "torch.Size([1, 3])\n",
      "['[CLS]', 'here', '[SEP]']\n",
      "<class 'list'>\n",
      "tensor([[[-0.2151,  0.0988, -0.0935,  ..., -0.2806,  0.2266,  0.3764],\n",
      "         [-0.6091, -0.2245, -0.8580,  ...,  0.2867,  0.6157, -0.5735],\n",
      "         [ 1.0697,  0.0667, -0.3977,  ...,  0.0703, -0.6323, -0.2769]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.encode(\"Here\", return_tensors=\"pt\")\n",
    "print(type(tokenized))\n",
    "print(tokenized)\n",
    "print(tokenized.shape)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized.squeeze(dim=0))\n",
    "print(tokens)\n",
    "print(type(tokens))\n",
    "embedded = model(input_ids=tokenized)\n",
    "last = embedded.last_hidden_state\n",
    "print(last)\n",
    "last\n",
    "print(last.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'is',\n",
       " 'my',\n",
       " 'text',\n",
       " ',',\n",
       " 'some',\n",
       " 'gi',\n",
       " '##bber',\n",
       " '##ish',\n",
       " ':',\n",
       " 'sd',\n",
       " '##sd',\n",
       " '##as',\n",
       " '##da',\n",
       " '##f',\n",
       " ',',\n",
       " 'and',\n",
       " 'animals',\n",
       " ',',\n",
       " 'hip',\n",
       " '##po',\n",
       " '##s',\n",
       " 'etc',\n",
       " '.',\n",
       " '[',\n",
       " '1',\n",
       " ']']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized.squeeze(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': tensor([[  101,  2182,  2003,  2026,  3793,  1010,  2070, 21025, 29325,  4509,\n",
      "          1024, 17371, 16150,  3022,  2850,  2546,  1010,  1998,  4176,  1010,\n",
      "          5099,  6873,  2015,  4385,  1012,  1031,  1015,  1033,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n",
      "tensor([[[-0.0725,  0.2891, -0.2205,  ..., -0.3550,  0.5276,  0.5011],\n",
      "         [-1.0338, -0.0576,  0.0736,  ..., -0.3047,  0.4641, -0.0979],\n",
      "         [-0.6666, -0.1912,  0.5514,  ...,  0.2056, -0.1143,  0.3831],\n",
      "         ...,\n",
      "         [-0.1139,  0.2144,  0.5290,  ..., -0.1144,  0.7179,  1.0353],\n",
      "         [ 0.0453,  0.3491,  0.3005,  ..., -0.6518, -0.2924,  0.1984],\n",
      "         [ 0.5270,  0.6090, -0.2633,  ...,  0.1840, -0.5477, -0.1830]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 29, 768])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(\"Here is my text, some gibberish: sdsdasdaf, and animals, hippos etc. [1]\", return_tensors=\"pt\")\n",
    "print(type(tokenized))\n",
    "print(tokenized)\n",
    "embedded = model(**tokenized)\n",
    "last = embedded.last_hidden_state\n",
    "print(last)\n",
    "print(last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "t = Tensor([1,2,3,4])\n",
    "print(t.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1., 2., 3., 4.]), tensor([1., 2., 3., 4.])]\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "stacked = torch.stack([t,t])\n",
    "print([t,t])\n",
    "print(stacked)\n",
    "print(stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 3])\n",
      "tensor([[[[-2.1386, -0.3540, -0.8361]],\n",
      "\n",
      "         [[-0.0905, -0.3460, -2.0942]],\n",
      "\n",
      "         [[-1.3964, -0.0758, -0.8706]]]])\n",
      "torch.Size([3, 3])\n",
      "tensor([[-2.1386, -0.3540, -0.8361],\n",
      "        [-0.0905, -0.3460, -2.0942],\n",
      "        [-1.3964, -0.0758, -0.8706]])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.randn(1,3,1,3)\n",
    "print(t2.shape)\n",
    "print(t2)\n",
    "squeezed = torch.squeeze(t2)\n",
    "print(squeezed.shape)\n",
    "print(squeezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.1386, -0.3540, -0.8361]],\n",
      "\n",
      "         [[-0.0905, -0.3460, -2.0942]],\n",
      "\n",
      "         [[-1.3964, -0.0758, -0.8706]]]])\n",
      "tensor([[[[-2.1386],\n",
      "          [-0.3540],\n",
      "          [-0.8361]]],\n",
      "\n",
      "\n",
      "        [[[-0.0905],\n",
      "          [-0.3460],\n",
      "          [-2.0942]]],\n",
      "\n",
      "\n",
      "        [[[-1.3964],\n",
      "          [-0.0758],\n",
      "          [-0.8706]]]])\n"
     ]
    }
   ],
   "source": [
    "print(t2.permute(0, 1, 2, 3))\n",
    "print(t2.permute(1, 2, 3, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vylet-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
